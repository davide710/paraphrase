{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11284104,"sourceType":"datasetVersion","datasetId":7055154}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:42:32.716663Z","iopub.execute_input":"2025-04-06T09:42:32.716946Z","iopub.status.idle":"2025-04-06T09:46:07.327160Z","shell.execute_reply.started":"2025-04-06T09:42:32.716915Z","shell.execute_reply":"2025-04-06T09:46:07.325918Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 2048\ndtype = None\nload_in_4bit = True\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:46:07.328391Z","iopub.execute_input":"2025-04-06T09:46:07.328690Z","iopub.status.idle":"2025-04-06T09:47:06.730728Z","shell.execute_reply.started":"2025-04-06T09:46:07.328666Z","shell.execute_reply":"2025-04-06T09:47:06.730050Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nUnsloth: Failed to patch Gemma3ForConditionalGeneration.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b0a4a3fd9148e2ace6246fff5ea21c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/198 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b553a3952a4a3a84b9f01bbf8a7ef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e1c6517d774e5a8cb4dbf7dfeb7bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"281093feacf34736ac4b34b0d5b3aff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec7fbe872d4243b6b93cb7db43a8ef46"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=42,\n    use_rslora=False,\n    loftq_config=None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:47:06.731507Z","iopub.execute_input":"2025-04-06T09:47:06.731750Z","iopub.status.idle":"2025-04-06T09:47:12.892112Z","shell.execute_reply.started":"2025-04-06T09:47:06.731729Z","shell.execute_reply":"2025-04-06T09:47:12.891158Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"with open('/kaggle/input/simplified-macbeth/simplified_macbeth', 'r') as f:\n    text = f.read()\n\nlines = text.split('\\n\\n')\nwithout_name = [''.join(l.split(':')[1:])[1:] for l in lines]\noriginals = [l.split('[[')[0][:-1] for l in without_name]\ntranslated = [l.split('[[')[1].split(']]')[0] for l in without_name]\nprint(originals[115], '---->', translated[115])\n\nimport pandas as pd\nfrom datasets import Dataset\n\ndata = {\"input\": originals, \"output\": translated}\ndf = pd.DataFrame(data)\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:47:12.894245Z","iopub.execute_input":"2025-04-06T09:47:12.894469Z","iopub.status.idle":"2025-04-06T09:47:15.547336Z","shell.execute_reply.started":"2025-04-06T09:47:12.894450Z","shell.execute_reply":"2025-04-06T09:47:15.546597Z"}},"outputs":[{"name":"stdout","text":"We will proceed no further in this business He hath honour'd me of late; and I have bought Golden opinions from all sorts of people, Which would be worn now in their newest gloss, Not cast aside so soon. ----> We will not continue with this plan. He has honored me recently, and I have earned the respect of all kinds of people, which I want to enjoy now, not throw away so soon.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"prompt = \"\"\"Below is an instruction on how to translate text from complex and archaic to simple and modern english.\nThe input contains this text. Write in the response a faithful translation.\n\n### Instruction:\nTranslate the following line from complex to simple english.\nIt may contain archaic and literary words or structures: make them easily understandable.\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\ndef formatting_prompts_func(data):\n    inputs = data['input']\n    outputs = data['output']\n    texts = []\n    for inp, outp in zip(inputs, outputs):\n        text = prompt.format(inp, outp) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\" : texts}\n\ndataset = dataset.map(formatting_prompts_func, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:47:15.548345Z","iopub.execute_input":"2025-04-06T09:47:15.548633Z","iopub.status.idle":"2025-04-06T09:47:15.591764Z","shell.execute_reply.started":"2025-04-06T09:47:15.548609Z","shell.execute_reply":"2025-04-06T09:47:15.590859Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"491976ed78db495996be77e0685cc8a8"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=5,\n        max_steps=60,#num_train_epochs=1\n        learning_rate=2e-4,\n        fp16=not torch.cuda.is_bf16_supported(),\n        bf16=torch.cuda.is_bf16_supported(),\n        logging_steps=1,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=42,\n        output_dir=\"outputs\",\n        report_to=\"none\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:47:15.592663Z","iopub.execute_input":"2025-04-06T09:47:15.592912Z","iopub.status.idle":"2025-04-06T09:47:17.618797Z","shell.execute_reply.started":"2025-04-06T09:47:15.592886Z","shell.execute_reply":"2025-04-06T09:47:17.617963Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/654 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7274dfa88d514aa28012ffa72ecda659"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"gpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:47:17.619767Z","iopub.execute_input":"2025-04-06T09:47:17.620095Z","iopub.status.idle":"2025-04-06T09:47:17.625817Z","shell.execute_reply.started":"2025-04-06T09:47:17.620070Z","shell.execute_reply":"2025-04-06T09:47:17.625081Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n7.117 GB of memory reserved.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:47:17.626758Z","iopub.execute_input":"2025-04-06T09:47:17.627083Z","iopub.status.idle":"2025-04-06T09:59:18.208290Z","shell.execute_reply.started":"2025-04-06T09:47:17.627053Z","shell.execute_reply":"2025-04-06T09:59:18.207491Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 654 | Num Epochs = 2 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 11:41, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.883800</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.868200</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.634300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.675200</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.423500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.974300</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.665000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.437400</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.245000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.977500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.963500</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.923300</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.690700</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.761100</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.791200</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.573800</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.573600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.668900</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.528400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.647000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.565000</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.563300</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.601900</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.670500</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.501800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.603900</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.592800</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.508700</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.558600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.573500</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.468000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.534200</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.544100</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.565700</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.673900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.588400</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.603000</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.361700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.593900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.501200</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.596000</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.371100</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.332700</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.448700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.396500</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.439800</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.424600</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.404400</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.346500</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.396500</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.426300</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.449600</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.401800</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.297000</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.375600</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.401400</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.396500</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.425500</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.297700</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.378500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T09:59:18.209106Z","iopub.execute_input":"2025-04-06T09:59:18.209327Z","iopub.status.idle":"2025-04-06T09:59:18.215911Z","shell.execute_reply.started":"2025-04-06T09:59:18.209308Z","shell.execute_reply":"2025-04-06T09:59:18.215199Z"}},"outputs":[{"name":"stdout","text":"11.98 minutes used for training.\nPeak reserved memory = 7.373 GB.\nPeak reserved memory for training = 0.256 GB.\nPeak reserved memory % of max memory = 50.017 %.\nPeak reserved memory for training % of max memory = 1.737 %.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)\ninputs = tokenizer(\n[\n    prompt.format(\n        \"This above all: to thine own self be true, And it must follow, as the night the day,Thou canst not then be false to any man.\",\n        \"\",\n    )\n], return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=2048, use_cache=True)\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T10:01:47.063628Z","iopub.execute_input":"2025-04-06T10:01:47.064018Z","iopub.status.idle":"2025-04-06T10:01:48.889373Z","shell.execute_reply.started":"2025-04-06T10:01:47.063991Z","shell.execute_reply":"2025-04-06T10:01:48.888647Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['<|begin_of_text|>Below is an instruction on how to translate text from complex and archaic to simple and modern english.\\nThe input contains this text. Write in the response a faithful translation.\\n\\n### Instruction:\\nTranslate the following line from complex to simple english.\\nIt may contain archaic and literary words or structures: make them easily understandable.\\n\\n### Input:\\nThis above all: to thine own self be true, And it must follow, as the night the day,Thou canst not then be false to any man.\\n\\n### Response:\\nAbove all else, be true to yourself, and it will naturally follow that you cannot be false to anyone else.<|end_of_text|>']"},"metadata":{}}],"execution_count":12}]}